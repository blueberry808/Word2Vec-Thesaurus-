{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:56.150406Z",
     "iopub.status.busy": "2025-11-20T00:27:56.150142Z",
     "iopub.status.idle": "2025-11-20T00:27:56.417806Z",
     "shell.execute_reply": "2025-11-20T00:27:56.417203Z",
     "shell.execute_reply.started": "2025-11-20T00:27:56.150388Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/brown-corpus/brown.csv\n",
      "/kaggle/input/brown-corpus/cats.csv\n",
      "/kaggle/input/brown-corpus/brown-meta.json\n",
      "/kaggle/input/brown-corpus/brown/brown/cb09\n",
      "/kaggle/input/brown-corpus/brown/brown/cb10\n",
      "/kaggle/input/brown-corpus/brown/brown/cj06\n",
      "/kaggle/input/brown-corpus/brown/brown/cd17\n",
      "/kaggle/input/brown-corpus/brown/brown/ca35\n",
      "/kaggle/input/brown-corpus/brown/brown/cg20\n",
      "/kaggle/input/brown-corpus/brown/brown/cf02\n",
      "/kaggle/input/brown-corpus/brown/brown/cc17\n",
      "/kaggle/input/brown-corpus/brown/brown/cl07\n",
      "/kaggle/input/brown-corpus/brown/brown/cj74\n",
      "/kaggle/input/brown-corpus/brown/brown/cg40\n",
      "/kaggle/input/brown-corpus/brown/brown/cl15\n",
      "/kaggle/input/brown-corpus/brown/brown/cj72\n",
      "/kaggle/input/brown-corpus/brown/brown/cm03\n",
      "/kaggle/input/brown-corpus/brown/brown/cc03\n",
      "/kaggle/input/brown-corpus/brown/brown/cg72\n",
      "/kaggle/input/brown-corpus/brown/brown/cj16\n",
      "/kaggle/input/brown-corpus/brown/brown/ch02\n",
      "/kaggle/input/brown-corpus/brown/brown/cg34\n",
      "/kaggle/input/brown-corpus/brown/brown/cj42\n",
      "/kaggle/input/brown-corpus/brown/brown/ck19\n",
      "/kaggle/input/brown-corpus/brown/brown/cn06\n",
      "/kaggle/input/brown-corpus/brown/brown/cf12\n",
      "/kaggle/input/brown-corpus/brown/brown/cg52\n",
      "/kaggle/input/brown-corpus/brown/brown/cl09\n",
      "/kaggle/input/brown-corpus/brown/brown/cg39\n",
      "/kaggle/input/brown-corpus/brown/brown/ce07\n",
      "/kaggle/input/brown-corpus/brown/brown/cj10\n",
      "/kaggle/input/brown-corpus/brown/brown/cg73\n",
      "/kaggle/input/brown-corpus/brown/brown/cj79\n",
      "/kaggle/input/brown-corpus/brown/brown/cm01\n",
      "/kaggle/input/brown-corpus/brown/brown/cb16\n",
      "/kaggle/input/brown-corpus/brown/brown/cf38\n",
      "/kaggle/input/brown-corpus/brown/brown/cc11\n",
      "/kaggle/input/brown-corpus/brown/brown/cb27\n",
      "/kaggle/input/brown-corpus/brown/brown/cj64\n",
      "/kaggle/input/brown-corpus/brown/brown/cd13\n",
      "/kaggle/input/brown-corpus/brown/brown/ck04\n",
      "/kaggle/input/brown-corpus/brown/brown/cf13\n",
      "/kaggle/input/brown-corpus/brown/brown/cl03\n",
      "/kaggle/input/brown-corpus/brown/brown/cj41\n",
      "/kaggle/input/brown-corpus/brown/brown/cf15\n",
      "/kaggle/input/brown-corpus/brown/brown/ce14\n",
      "/kaggle/input/brown-corpus/brown/brown/cn24\n",
      "/kaggle/input/brown-corpus/brown/brown/cf41\n",
      "/kaggle/input/brown-corpus/brown/brown/ce05\n",
      "/kaggle/input/brown-corpus/brown/brown/cg05\n",
      "/kaggle/input/brown-corpus/brown/brown/cb12\n",
      "/kaggle/input/brown-corpus/brown/brown/cj75\n",
      "/kaggle/input/brown-corpus/brown/brown/cl18\n",
      "/kaggle/input/brown-corpus/brown/brown/cd05\n",
      "/kaggle/input/brown-corpus/brown/brown/cg06\n",
      "/kaggle/input/brown-corpus/brown/brown/cb18\n",
      "/kaggle/input/brown-corpus/brown/brown/ch08\n",
      "/kaggle/input/brown-corpus/brown/brown/cf39\n",
      "/kaggle/input/brown-corpus/brown/brown/ck05\n",
      "/kaggle/input/brown-corpus/brown/brown/ce24\n",
      "/kaggle/input/brown-corpus/brown/brown/ch03\n",
      "/kaggle/input/brown-corpus/brown/brown/cb25\n",
      "/kaggle/input/brown-corpus/brown/brown/ce27\n",
      "/kaggle/input/brown-corpus/brown/brown/cr07\n",
      "/kaggle/input/brown-corpus/brown/brown/cj17\n",
      "/kaggle/input/brown-corpus/brown/brown/cr02\n",
      "/kaggle/input/brown-corpus/brown/brown/ch09\n",
      "/kaggle/input/brown-corpus/brown/brown/cp02\n",
      "/kaggle/input/brown-corpus/brown/brown/cj36\n",
      "/kaggle/input/brown-corpus/brown/brown/cn22\n",
      "/kaggle/input/brown-corpus/brown/brown/cj49\n",
      "/kaggle/input/brown-corpus/brown/brown/ch07\n",
      "/kaggle/input/brown-corpus/brown/brown/ca09\n",
      "/kaggle/input/brown-corpus/brown/brown/cf40\n",
      "/kaggle/input/brown-corpus/brown/brown/cj23\n",
      "/kaggle/input/brown-corpus/brown/brown/ck02\n",
      "/kaggle/input/brown-corpus/brown/brown/cj71\n",
      "/kaggle/input/brown-corpus/brown/brown/cp21\n",
      "/kaggle/input/brown-corpus/brown/brown/cj43\n",
      "/kaggle/input/brown-corpus/brown/brown/cj46\n",
      "/kaggle/input/brown-corpus/brown/brown/ch11\n",
      "/kaggle/input/brown-corpus/brown/brown/ce31\n",
      "/kaggle/input/brown-corpus/brown/brown/cn03\n",
      "/kaggle/input/brown-corpus/brown/brown/cd04\n",
      "/kaggle/input/brown-corpus/brown/brown/cg53\n",
      "/kaggle/input/brown-corpus/brown/brown/ca26\n",
      "/kaggle/input/brown-corpus/brown/brown/cm04\n",
      "/kaggle/input/brown-corpus/brown/brown/ck16\n",
      "/kaggle/input/brown-corpus/brown/brown/cb02\n",
      "/kaggle/input/brown-corpus/brown/brown/cg70\n",
      "/kaggle/input/brown-corpus/brown/brown/cb19\n",
      "/kaggle/input/brown-corpus/brown/brown/ch25\n",
      "/kaggle/input/brown-corpus/brown/brown/cj65\n",
      "/kaggle/input/brown-corpus/brown/brown/ck07\n",
      "/kaggle/input/brown-corpus/brown/brown/cj31\n",
      "/kaggle/input/brown-corpus/brown/brown/cf44\n",
      "/kaggle/input/brown-corpus/brown/brown/cg49\n",
      "/kaggle/input/brown-corpus/brown/brown/cg58\n",
      "/kaggle/input/brown-corpus/brown/brown/cj55\n",
      "/kaggle/input/brown-corpus/brown/brown/cg27\n",
      "/kaggle/input/brown-corpus/brown/brown/ce09\n",
      "/kaggle/input/brown-corpus/brown/brown/cg69\n",
      "/kaggle/input/brown-corpus/brown/brown/cl16\n",
      "/kaggle/input/brown-corpus/brown/brown/cg61\n",
      "/kaggle/input/brown-corpus/brown/brown/cn07\n",
      "/kaggle/input/brown-corpus/brown/brown/cg29\n",
      "/kaggle/input/brown-corpus/brown/brown/cg03\n",
      "/kaggle/input/brown-corpus/brown/brown/cj18\n",
      "/kaggle/input/brown-corpus/brown/brown/ca07\n",
      "/kaggle/input/brown-corpus/brown/brown/cf04\n",
      "/kaggle/input/brown-corpus/brown/brown/ck14\n",
      "/kaggle/input/brown-corpus/brown/brown/cl08\n",
      "/kaggle/input/brown-corpus/brown/brown/cg08\n",
      "/kaggle/input/brown-corpus/brown/brown/cn09\n",
      "/kaggle/input/brown-corpus/brown/brown/cd11\n",
      "/kaggle/input/brown-corpus/brown/brown/ce02\n",
      "/kaggle/input/brown-corpus/brown/brown/cf01\n",
      "/kaggle/input/brown-corpus/brown/brown/cc06\n",
      "/kaggle/input/brown-corpus/brown/brown/cj77\n",
      "/kaggle/input/brown-corpus/brown/brown/ca40\n",
      "/kaggle/input/brown-corpus/brown/brown/cn01\n",
      "/kaggle/input/brown-corpus/brown/brown/cr06\n",
      "/kaggle/input/brown-corpus/brown/brown/cg46\n",
      "/kaggle/input/brown-corpus/brown/brown/cg16\n",
      "/kaggle/input/brown-corpus/brown/brown/cc09\n",
      "/kaggle/input/brown-corpus/brown/brown/cj76\n",
      "/kaggle/input/brown-corpus/brown/brown/ca08\n",
      "/kaggle/input/brown-corpus/brown/brown/ca18\n",
      "/kaggle/input/brown-corpus/brown/brown/cg51\n",
      "/kaggle/input/brown-corpus/brown/brown/cb11\n",
      "/kaggle/input/brown-corpus/brown/brown/ce06\n",
      "/kaggle/input/brown-corpus/brown/brown/ck10\n",
      "/kaggle/input/brown-corpus/brown/brown/ce10\n",
      "/kaggle/input/brown-corpus/brown/brown/ck01\n",
      "/kaggle/input/brown-corpus/brown/brown/cj37\n",
      "/kaggle/input/brown-corpus/brown/brown/cc14\n",
      "/kaggle/input/brown-corpus/brown/brown/cl21\n",
      "/kaggle/input/brown-corpus/brown/brown/cb24\n",
      "/kaggle/input/brown-corpus/brown/brown/cp26\n",
      "/kaggle/input/brown-corpus/brown/brown/cj29\n",
      "/kaggle/input/brown-corpus/brown/brown/cf22\n",
      "/kaggle/input/brown-corpus/brown/brown/ca24\n",
      "/kaggle/input/brown-corpus/brown/brown/ce20\n",
      "/kaggle/input/brown-corpus/brown/brown/ca30\n",
      "/kaggle/input/brown-corpus/brown/brown/cf33\n",
      "/kaggle/input/brown-corpus/brown/brown/cn04\n",
      "/kaggle/input/brown-corpus/brown/brown/cj05\n",
      "/kaggle/input/brown-corpus/brown/brown/cb21\n",
      "/kaggle/input/brown-corpus/brown/brown/cp06\n",
      "/kaggle/input/brown-corpus/brown/brown/cc02\n",
      "/kaggle/input/brown-corpus/brown/brown/cj12\n",
      "/kaggle/input/brown-corpus/brown/brown/cats.txt\n",
      "/kaggle/input/brown-corpus/brown/brown/cb04\n",
      "/kaggle/input/brown-corpus/brown/brown/ca12\n",
      "/kaggle/input/brown-corpus/brown/brown/cf48\n",
      "/kaggle/input/brown-corpus/brown/brown/cf42\n",
      "/kaggle/input/brown-corpus/brown/brown/cb23\n",
      "/kaggle/input/brown-corpus/brown/brown/README\n",
      "/kaggle/input/brown-corpus/brown/brown/cj78\n",
      "/kaggle/input/brown-corpus/brown/brown/ce18\n",
      "/kaggle/input/brown-corpus/brown/brown/cg66\n",
      "/kaggle/input/brown-corpus/brown/brown/cf11\n",
      "/kaggle/input/brown-corpus/brown/brown/cn08\n",
      "/kaggle/input/brown-corpus/brown/brown/cg25\n",
      "/kaggle/input/brown-corpus/brown/brown/cc05\n",
      "/kaggle/input/brown-corpus/brown/brown/cd15\n",
      "/kaggle/input/brown-corpus/brown/brown/cp28\n",
      "/kaggle/input/brown-corpus/brown/brown/cp16\n",
      "/kaggle/input/brown-corpus/brown/brown/cg36\n",
      "/kaggle/input/brown-corpus/brown/brown/cg37\n",
      "/kaggle/input/brown-corpus/brown/brown/cj58\n",
      "/kaggle/input/brown-corpus/brown/brown/cp12\n",
      "/kaggle/input/brown-corpus/brown/brown/cg10\n",
      "/kaggle/input/brown-corpus/brown/brown/cl06\n",
      "/kaggle/input/brown-corpus/brown/brown/cj08\n",
      "/kaggle/input/brown-corpus/brown/brown/ch01\n",
      "/kaggle/input/brown-corpus/brown/brown/cl01\n",
      "/kaggle/input/brown-corpus/brown/brown/cf09\n",
      "/kaggle/input/brown-corpus/brown/brown/ch24\n",
      "/kaggle/input/brown-corpus/brown/brown/cc04\n",
      "/kaggle/input/brown-corpus/brown/brown/ch28\n",
      "/kaggle/input/brown-corpus/brown/brown/ck29\n",
      "/kaggle/input/brown-corpus/brown/brown/cj60\n",
      "/kaggle/input/brown-corpus/brown/brown/cg55\n",
      "/kaggle/input/brown-corpus/brown/brown/cg35\n",
      "/kaggle/input/brown-corpus/brown/brown/cf26\n",
      "/kaggle/input/brown-corpus/brown/brown/ca02\n",
      "/kaggle/input/brown-corpus/brown/brown/ch13\n",
      "/kaggle/input/brown-corpus/brown/brown/cj11\n",
      "/kaggle/input/brown-corpus/brown/brown/cj53\n",
      "/kaggle/input/brown-corpus/brown/brown/cg56\n",
      "/kaggle/input/brown-corpus/brown/brown/ca05\n",
      "/kaggle/input/brown-corpus/brown/brown/cg32\n",
      "/kaggle/input/brown-corpus/brown/brown/ch30\n",
      "/kaggle/input/brown-corpus/brown/brown/ch05\n",
      "/kaggle/input/brown-corpus/brown/brown/cd10\n",
      "/kaggle/input/brown-corpus/brown/brown/cp25\n",
      "/kaggle/input/brown-corpus/brown/brown/cj67\n",
      "/kaggle/input/brown-corpus/brown/brown/ce12\n",
      "/kaggle/input/brown-corpus/brown/brown/ce28\n",
      "/kaggle/input/brown-corpus/brown/brown/cp03\n",
      "/kaggle/input/brown-corpus/brown/brown/cn02\n",
      "/kaggle/input/brown-corpus/brown/brown/cj54\n",
      "/kaggle/input/brown-corpus/brown/brown/cp15\n",
      "/kaggle/input/brown-corpus/brown/brown/cj62\n",
      "/kaggle/input/brown-corpus/brown/brown/ce33\n",
      "/kaggle/input/brown-corpus/brown/brown/cp18\n",
      "/kaggle/input/brown-corpus/brown/brown/ch17\n",
      "/kaggle/input/brown-corpus/brown/brown/cj07\n",
      "/kaggle/input/brown-corpus/brown/brown/cb22\n",
      "/kaggle/input/brown-corpus/brown/brown/ck13\n",
      "/kaggle/input/brown-corpus/brown/brown/ca37\n",
      "/kaggle/input/brown-corpus/brown/brown/cj57\n",
      "/kaggle/input/brown-corpus/brown/brown/cf25\n",
      "/kaggle/input/brown-corpus/brown/brown/cb26\n",
      "/kaggle/input/brown-corpus/brown/brown/cd09\n",
      "/kaggle/input/brown-corpus/brown/brown/cg33\n",
      "/kaggle/input/brown-corpus/brown/brown/ca25\n",
      "/kaggle/input/brown-corpus/brown/brown/cj28\n",
      "/kaggle/input/brown-corpus/brown/brown/ca16\n",
      "/kaggle/input/brown-corpus/brown/brown/cj03\n",
      "/kaggle/input/brown-corpus/brown/brown/cn26\n",
      "/kaggle/input/brown-corpus/brown/brown/cf27\n",
      "/kaggle/input/brown-corpus/brown/brown/ce16\n",
      "/kaggle/input/brown-corpus/brown/brown/cj02\n",
      "/kaggle/input/brown-corpus/brown/brown/cf21\n",
      "/kaggle/input/brown-corpus/brown/brown/cl22\n",
      "/kaggle/input/brown-corpus/brown/brown/ck17\n",
      "/kaggle/input/brown-corpus/brown/brown/cj20\n",
      "/kaggle/input/brown-corpus/brown/brown/ce30\n",
      "/kaggle/input/brown-corpus/brown/brown/cg45\n",
      "/kaggle/input/brown-corpus/brown/brown/cp07\n",
      "/kaggle/input/brown-corpus/brown/brown/cj19\n",
      "/kaggle/input/brown-corpus/brown/brown/cp01\n",
      "/kaggle/input/brown-corpus/brown/brown/ch27\n",
      "/kaggle/input/brown-corpus/brown/brown/cg30\n",
      "/kaggle/input/brown-corpus/brown/brown/ce35\n",
      "/kaggle/input/brown-corpus/brown/brown/cj09\n",
      "/kaggle/input/brown-corpus/brown/brown/cp17\n",
      "/kaggle/input/brown-corpus/brown/brown/ca27\n",
      "/kaggle/input/brown-corpus/brown/brown/cg24\n",
      "/kaggle/input/brown-corpus/brown/brown/cg42\n",
      "/kaggle/input/brown-corpus/brown/brown/ce26\n",
      "/kaggle/input/brown-corpus/brown/brown/ce17\n",
      "/kaggle/input/brown-corpus/brown/brown/cj50\n",
      "/kaggle/input/brown-corpus/brown/brown/cp29\n",
      "/kaggle/input/brown-corpus/brown/brown/cj48\n",
      "/kaggle/input/brown-corpus/brown/brown/cj63\n",
      "/kaggle/input/brown-corpus/brown/brown/cf43\n",
      "/kaggle/input/brown-corpus/brown/brown/cf37\n",
      "/kaggle/input/brown-corpus/brown/brown/ce36\n",
      "/kaggle/input/brown-corpus/brown/brown/ck23\n",
      "/kaggle/input/brown-corpus/brown/brown/cf28\n",
      "/kaggle/input/brown-corpus/brown/brown/ch22\n",
      "/kaggle/input/brown-corpus/brown/brown/cg62\n",
      "/kaggle/input/brown-corpus/brown/brown/cg50\n",
      "/kaggle/input/brown-corpus/brown/brown/cf05\n",
      "/kaggle/input/brown-corpus/brown/brown/cf08\n",
      "/kaggle/input/brown-corpus/brown/brown/ca20\n",
      "/kaggle/input/brown-corpus/brown/brown/cg68\n",
      "/kaggle/input/brown-corpus/brown/brown/ck11\n",
      "/kaggle/input/brown-corpus/brown/brown/ck03\n",
      "/kaggle/input/brown-corpus/brown/brown/ch14\n",
      "/kaggle/input/brown-corpus/brown/brown/ce23\n",
      "/kaggle/input/brown-corpus/brown/brown/cj24\n",
      "/kaggle/input/brown-corpus/brown/brown/cj32\n",
      "/kaggle/input/brown-corpus/brown/brown/cp19\n",
      "/kaggle/input/brown-corpus/brown/brown/cn20\n",
      "/kaggle/input/brown-corpus/brown/brown/cg12\n",
      "/kaggle/input/brown-corpus/brown/brown/cf30\n",
      "/kaggle/input/brown-corpus/brown/brown/cl11\n",
      "/kaggle/input/brown-corpus/brown/brown/ce22\n",
      "/kaggle/input/brown-corpus/brown/brown/cg71\n",
      "/kaggle/input/brown-corpus/brown/brown/cn19\n",
      "/kaggle/input/brown-corpus/brown/brown/cg17\n",
      "/kaggle/input/brown-corpus/brown/brown/ck06\n",
      "/kaggle/input/brown-corpus/brown/brown/cj52\n",
      "/kaggle/input/brown-corpus/brown/brown/cf46\n",
      "/kaggle/input/brown-corpus/brown/brown/cr03\n",
      "/kaggle/input/brown-corpus/brown/brown/ca21\n",
      "/kaggle/input/brown-corpus/brown/brown/cj35\n",
      "/kaggle/input/brown-corpus/brown/brown/cf17\n",
      "/kaggle/input/brown-corpus/brown/brown/cp09\n",
      "/kaggle/input/brown-corpus/brown/brown/ca28\n",
      "/kaggle/input/brown-corpus/brown/brown/cj45\n",
      "/kaggle/input/brown-corpus/brown/brown/cj73\n",
      "/kaggle/input/brown-corpus/brown/brown/ca15\n",
      "/kaggle/input/brown-corpus/brown/brown/cl02\n",
      "/kaggle/input/brown-corpus/brown/brown/cn15\n",
      "/kaggle/input/brown-corpus/brown/brown/cg26\n",
      "/kaggle/input/brown-corpus/brown/brown/cf06\n",
      "/kaggle/input/brown-corpus/brown/brown/cf34\n",
      "/kaggle/input/brown-corpus/brown/brown/cg59\n",
      "/kaggle/input/brown-corpus/brown/brown/cp05\n",
      "/kaggle/input/brown-corpus/brown/brown/cb05\n",
      "/kaggle/input/brown-corpus/brown/brown/cj27\n",
      "/kaggle/input/brown-corpus/brown/brown/ck25\n",
      "/kaggle/input/brown-corpus/brown/brown/cn28\n",
      "/kaggle/input/brown-corpus/brown/brown/ca01\n",
      "/kaggle/input/brown-corpus/brown/brown/cd01\n",
      "/kaggle/input/brown-corpus/brown/brown/cg11\n",
      "/kaggle/input/brown-corpus/brown/brown/cn16\n",
      "/kaggle/input/brown-corpus/brown/brown/ck26\n",
      "/kaggle/input/brown-corpus/brown/brown/cg07\n",
      "/kaggle/input/brown-corpus/brown/brown/cn13\n",
      "/kaggle/input/brown-corpus/brown/brown/cb01\n",
      "/kaggle/input/brown-corpus/brown/brown/cj44\n",
      "/kaggle/input/brown-corpus/brown/brown/cj22\n",
      "/kaggle/input/brown-corpus/brown/brown/ce13\n",
      "/kaggle/input/brown-corpus/brown/brown/cn29\n",
      "/kaggle/input/brown-corpus/brown/brown/cc08\n",
      "/kaggle/input/brown-corpus/brown/brown/cf36\n",
      "/kaggle/input/brown-corpus/brown/brown/ca13\n",
      "/kaggle/input/brown-corpus/brown/brown/cj39\n",
      "/kaggle/input/brown-corpus/brown/brown/ck09\n",
      "/kaggle/input/brown-corpus/brown/brown/ca29\n",
      "/kaggle/input/brown-corpus/brown/brown/cp20\n",
      "/kaggle/input/brown-corpus/brown/brown/cn23\n",
      "/kaggle/input/brown-corpus/brown/brown/cf31\n",
      "/kaggle/input/brown-corpus/brown/brown/cb07\n",
      "/kaggle/input/brown-corpus/brown/brown/cg41\n",
      "/kaggle/input/brown-corpus/brown/brown/ck20\n",
      "/kaggle/input/brown-corpus/brown/brown/cj56\n",
      "/kaggle/input/brown-corpus/brown/brown/ca41\n",
      "/kaggle/input/brown-corpus/brown/brown/cg19\n",
      "/kaggle/input/brown-corpus/brown/brown/cl05\n",
      "/kaggle/input/brown-corpus/brown/brown/cg47\n",
      "/kaggle/input/brown-corpus/brown/brown/ca39\n",
      "/kaggle/input/brown-corpus/brown/brown/cl04\n",
      "/kaggle/input/brown-corpus/brown/brown/cn11\n",
      "/kaggle/input/brown-corpus/brown/brown/cr09\n",
      "/kaggle/input/brown-corpus/brown/brown/ck21\n",
      "/kaggle/input/brown-corpus/brown/brown/CONTENTS\n",
      "/kaggle/input/brown-corpus/brown/brown/cp08\n",
      "/kaggle/input/brown-corpus/brown/brown/ca31\n",
      "/kaggle/input/brown-corpus/brown/brown/cj59\n",
      "/kaggle/input/brown-corpus/brown/brown/cb20\n",
      "/kaggle/input/brown-corpus/brown/brown/cg15\n",
      "/kaggle/input/brown-corpus/brown/brown/cc07\n",
      "/kaggle/input/brown-corpus/brown/brown/cj61\n",
      "/kaggle/input/brown-corpus/brown/brown/ch16\n",
      "/kaggle/input/brown-corpus/brown/brown/cg21\n",
      "/kaggle/input/brown-corpus/brown/brown/cg75\n",
      "/kaggle/input/brown-corpus/brown/brown/cb17\n",
      "/kaggle/input/brown-corpus/brown/brown/cd02\n",
      "/kaggle/input/brown-corpus/brown/brown/ch15\n",
      "/kaggle/input/brown-corpus/brown/brown/ch20\n",
      "/kaggle/input/brown-corpus/brown/brown/cr01\n",
      "/kaggle/input/brown-corpus/brown/brown/cp23\n",
      "/kaggle/input/brown-corpus/brown/brown/cf14\n",
      "/kaggle/input/brown-corpus/brown/brown/cp13\n",
      "/kaggle/input/brown-corpus/brown/brown/cj40\n",
      "/kaggle/input/brown-corpus/brown/brown/ca03\n",
      "/kaggle/input/brown-corpus/brown/brown/cf20\n",
      "/kaggle/input/brown-corpus/brown/brown/cb13\n",
      "/kaggle/input/brown-corpus/brown/brown/cp11\n",
      "/kaggle/input/brown-corpus/brown/brown/cd07\n",
      "/kaggle/input/brown-corpus/brown/brown/ce11\n",
      "/kaggle/input/brown-corpus/brown/brown/cj69\n",
      "/kaggle/input/brown-corpus/brown/brown/cf16\n",
      "/kaggle/input/brown-corpus/brown/brown/cc01\n",
      "/kaggle/input/brown-corpus/brown/brown/cg31\n",
      "/kaggle/input/brown-corpus/brown/brown/cg22\n",
      "/kaggle/input/brown-corpus/brown/brown/cn25\n",
      "/kaggle/input/brown-corpus/brown/brown/cf47\n",
      "/kaggle/input/brown-corpus/brown/brown/ce21\n",
      "/kaggle/input/brown-corpus/brown/brown/ch29\n",
      "/kaggle/input/brown-corpus/brown/brown/cg14\n",
      "/kaggle/input/brown-corpus/brown/brown/cn21\n",
      "/kaggle/input/brown-corpus/brown/brown/cf29\n",
      "/kaggle/input/brown-corpus/brown/brown/ca04\n",
      "/kaggle/input/brown-corpus/brown/brown/ca10\n",
      "/kaggle/input/brown-corpus/brown/brown/cj25\n",
      "/kaggle/input/brown-corpus/brown/brown/cj15\n",
      "/kaggle/input/brown-corpus/brown/brown/cn27\n",
      "/kaggle/input/brown-corpus/brown/brown/cl23\n",
      "/kaggle/input/brown-corpus/brown/brown/cg44\n",
      "/kaggle/input/brown-corpus/brown/brown/cl14\n",
      "/kaggle/input/brown-corpus/brown/brown/cd14\n",
      "/kaggle/input/brown-corpus/brown/brown/cf24\n",
      "/kaggle/input/brown-corpus/brown/brown/cm02\n",
      "/kaggle/input/brown-corpus/brown/brown/cj13\n",
      "/kaggle/input/brown-corpus/brown/brown/cg63\n",
      "/kaggle/input/brown-corpus/brown/brown/cf23\n",
      "/kaggle/input/brown-corpus/brown/brown/cg28\n",
      "/kaggle/input/brown-corpus/brown/brown/ce32\n",
      "/kaggle/input/brown-corpus/brown/brown/ch18\n",
      "/kaggle/input/brown-corpus/brown/brown/cl12\n",
      "/kaggle/input/brown-corpus/brown/brown/cb08\n",
      "/kaggle/input/brown-corpus/brown/brown/cg64\n",
      "/kaggle/input/brown-corpus/brown/brown/cj14\n",
      "/kaggle/input/brown-corpus/brown/brown/ce15\n",
      "/kaggle/input/brown-corpus/brown/brown/cc15\n",
      "/kaggle/input/brown-corpus/brown/brown/cj66\n",
      "/kaggle/input/brown-corpus/brown/brown/cb03\n",
      "/kaggle/input/brown-corpus/brown/brown/cf10\n",
      "/kaggle/input/brown-corpus/brown/brown/cj38\n",
      "/kaggle/input/brown-corpus/brown/brown/cj04\n",
      "/kaggle/input/brown-corpus/brown/brown/cj30\n",
      "/kaggle/input/brown-corpus/brown/brown/cp22\n",
      "/kaggle/input/brown-corpus/brown/brown/cm06\n",
      "/kaggle/input/brown-corpus/brown/brown/ck08\n",
      "/kaggle/input/brown-corpus/brown/brown/cp27\n",
      "/kaggle/input/brown-corpus/brown/brown/cc16\n",
      "/kaggle/input/brown-corpus/brown/brown/ch12\n",
      "/kaggle/input/brown-corpus/brown/brown/cg65\n",
      "/kaggle/input/brown-corpus/brown/brown/cg23\n",
      "/kaggle/input/brown-corpus/brown/brown/ca17\n",
      "/kaggle/input/brown-corpus/brown/brown/ca34\n",
      "/kaggle/input/brown-corpus/brown/brown/cd16\n",
      "/kaggle/input/brown-corpus/brown/brown/cg18\n",
      "/kaggle/input/brown-corpus/brown/brown/ca23\n",
      "/kaggle/input/brown-corpus/brown/brown/cn12\n",
      "/kaggle/input/brown-corpus/brown/brown/cl13\n",
      "/kaggle/input/brown-corpus/brown/brown/cf03\n",
      "/kaggle/input/brown-corpus/brown/brown/ch19\n",
      "/kaggle/input/brown-corpus/brown/brown/cj33\n",
      "/kaggle/input/brown-corpus/brown/brown/ce03\n",
      "/kaggle/input/brown-corpus/brown/brown/ce25\n",
      "/kaggle/input/brown-corpus/brown/brown/cj01\n",
      "/kaggle/input/brown-corpus/brown/brown/cn17\n",
      "/kaggle/input/brown-corpus/brown/brown/ch26\n",
      "/kaggle/input/brown-corpus/brown/brown/ck12\n",
      "/kaggle/input/brown-corpus/brown/brown/ch23\n",
      "/kaggle/input/brown-corpus/brown/brown/ce08\n",
      "/kaggle/input/brown-corpus/brown/brown/cf07\n",
      "/kaggle/input/brown-corpus/brown/brown/cr05\n",
      "/kaggle/input/brown-corpus/brown/brown/ce34\n",
      "/kaggle/input/brown-corpus/brown/brown/cg04\n",
      "/kaggle/input/brown-corpus/brown/brown/cg57\n",
      "/kaggle/input/brown-corpus/brown/brown/ck22\n",
      "/kaggle/input/brown-corpus/brown/brown/cp14\n",
      "/kaggle/input/brown-corpus/brown/brown/ck28\n",
      "/kaggle/input/brown-corpus/brown/brown/cg38\n",
      "/kaggle/input/brown-corpus/brown/brown/cl17\n",
      "/kaggle/input/brown-corpus/brown/brown/cf32\n",
      "/kaggle/input/brown-corpus/brown/brown/ca42\n",
      "/kaggle/input/brown-corpus/brown/brown/cm05\n",
      "/kaggle/input/brown-corpus/brown/brown/cn18\n",
      "/kaggle/input/brown-corpus/brown/brown/cd03\n",
      "/kaggle/input/brown-corpus/brown/brown/ca33\n",
      "/kaggle/input/brown-corpus/brown/brown/cc13\n",
      "/kaggle/input/brown-corpus/brown/brown/cj80\n",
      "/kaggle/input/brown-corpus/brown/brown/ch10\n",
      "/kaggle/input/brown-corpus/brown/brown/cd08\n",
      "/kaggle/input/brown-corpus/brown/brown/ce04\n",
      "/kaggle/input/brown-corpus/brown/brown/cl24\n",
      "/kaggle/input/brown-corpus/brown/brown/cp24\n",
      "/kaggle/input/brown-corpus/brown/brown/cg67\n",
      "/kaggle/input/brown-corpus/brown/brown/ca14\n",
      "/kaggle/input/brown-corpus/brown/brown/cn14\n",
      "/kaggle/input/brown-corpus/brown/brown/cg43\n",
      "/kaggle/input/brown-corpus/brown/brown/cb15\n",
      "/kaggle/input/brown-corpus/brown/brown/ca38\n",
      "/kaggle/input/brown-corpus/brown/brown/cf35\n",
      "/kaggle/input/brown-corpus/brown/brown/cj21\n",
      "/kaggle/input/brown-corpus/brown/brown/cj70\n",
      "/kaggle/input/brown-corpus/brown/brown/cg74\n",
      "/kaggle/input/brown-corpus/brown/brown/cg54\n",
      "/kaggle/input/brown-corpus/brown/brown/cg09\n",
      "/kaggle/input/brown-corpus/brown/brown/ca32\n",
      "/kaggle/input/brown-corpus/brown/brown/cg02\n",
      "/kaggle/input/brown-corpus/brown/brown/ce19\n",
      "/kaggle/input/brown-corpus/brown/brown/ca11\n",
      "/kaggle/input/brown-corpus/brown/brown/cn05\n",
      "/kaggle/input/brown-corpus/brown/brown/cp04\n",
      "/kaggle/input/brown-corpus/brown/brown/ce29\n",
      "/kaggle/input/brown-corpus/brown/brown/ck18\n",
      "/kaggle/input/brown-corpus/brown/brown/ca36\n",
      "/kaggle/input/brown-corpus/brown/brown/cb06\n",
      "/kaggle/input/brown-corpus/brown/brown/ck27\n",
      "/kaggle/input/brown-corpus/brown/brown/ch06\n",
      "/kaggle/input/brown-corpus/brown/brown/ck24\n",
      "/kaggle/input/brown-corpus/brown/brown/ca43\n",
      "/kaggle/input/brown-corpus/brown/brown/cg01\n",
      "/kaggle/input/brown-corpus/brown/brown/cg48\n",
      "/kaggle/input/brown-corpus/brown/brown/cf45\n",
      "/kaggle/input/brown-corpus/brown/brown/ck15\n",
      "/kaggle/input/brown-corpus/brown/brown/ce01\n",
      "/kaggle/input/brown-corpus/brown/brown/cg60\n",
      "/kaggle/input/brown-corpus/brown/brown/cj26\n",
      "/kaggle/input/brown-corpus/brown/brown/cg13\n",
      "/kaggle/input/brown-corpus/brown/brown/cp10\n",
      "/kaggle/input/brown-corpus/brown/brown/cd12\n",
      "/kaggle/input/brown-corpus/brown/brown/cc12\n",
      "/kaggle/input/brown-corpus/brown/brown/cj68\n",
      "/kaggle/input/brown-corpus/brown/brown/cd06\n",
      "/kaggle/input/brown-corpus/brown/brown/ca06\n",
      "/kaggle/input/brown-corpus/brown/brown/ca44\n",
      "/kaggle/input/brown-corpus/brown/brown/ca22\n",
      "/kaggle/input/brown-corpus/brown/brown/cj34\n",
      "/kaggle/input/brown-corpus/brown/brown/ca19\n",
      "/kaggle/input/brown-corpus/brown/brown/cl19\n",
      "/kaggle/input/brown-corpus/brown/brown/cr08\n",
      "/kaggle/input/brown-corpus/brown/brown/cf19\n",
      "/kaggle/input/brown-corpus/brown/brown/cc10\n",
      "/kaggle/input/brown-corpus/brown/brown/cf18\n",
      "/kaggle/input/brown-corpus/brown/brown/cr04\n",
      "/kaggle/input/brown-corpus/brown/brown/ch21\n",
      "/kaggle/input/brown-corpus/brown/brown/cj51\n",
      "/kaggle/input/brown-corpus/brown/brown/cl20\n",
      "/kaggle/input/brown-corpus/brown/brown/cl10\n",
      "/kaggle/input/brown-corpus/brown/brown/cj47\n",
      "/kaggle/input/brown-corpus/brown/brown/cb14\n",
      "/kaggle/input/brown-corpus/brown/brown/cn10\n",
      "/kaggle/input/brown-corpus/brown/brown/ch04\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:56.419241Z",
     "iopub.status.busy": "2025-11-20T00:27:56.419016Z",
     "iopub.status.idle": "2025-11-20T00:27:56.707599Z",
     "shell.execute_reply": "2025-11-20T00:27:56.706966Z",
     "shell.execute_reply.started": "2025-11-20T00:27:56.419225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "brown_df = pd.read_csv(\"/kaggle/input/brown-corpus/brown.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:56.708612Z",
     "iopub.status.busy": "2025-11-20T00:27:56.708383Z",
     "iopub.status.idle": "2025-11-20T00:27:56.718716Z",
     "shell.execute_reply": "2025-11-20T00:27:56.717957Z",
     "shell.execute_reply.started": "2025-11-20T00:27:56.708595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>para_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_pos</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Furthermore/rb ,/, as/cs an/at encouragement/n...</td>\n",
       "      <td>Furthermore , as an encouragement to revisioni...</td>\n",
       "      <td>rb , cs at nn in nn nn , pps rb bez jj to vb c...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The/at Unitarian/jj clergy/nns were/bed an/at ...</td>\n",
       "      <td>The Unitarian clergy were an exclusive club of...</td>\n",
       "      <td>at jj nns bed at jj nn in vbn nns -- cs at nn ...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Ezra/np Stiles/np Gannett/np ,/, an/at honorab...</td>\n",
       "      <td>Ezra Stiles Gannett , an honorable representat...</td>\n",
       "      <td>np np np , at jj nn in at nn , vbd ppl rb in a...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Even/rb so/rb ,/, Gannett/np judiciously/rb ar...</td>\n",
       "      <td>Even so , Gannett judiciously argued , the Ass...</td>\n",
       "      <td>rb rb , np rb vbd , at nn-tl md rb vb cs np ``...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>We/ppss today/nr are/ber not/* entitled/vbn to...</td>\n",
       "      <td>We today are not entitled to excoriate honest ...</td>\n",
       "      <td>ppss nr ber * vbn to vb jj nns wps vbd np to b...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  para_id  sent_id  \\\n",
       "0     cd05        0        0   \n",
       "1     cd05        0        1   \n",
       "2     cd05        0        2   \n",
       "3     cd05        0        3   \n",
       "4     cd05        0        4   \n",
       "\n",
       "                                            raw_text  \\\n",
       "0  Furthermore/rb ,/, as/cs an/at encouragement/n...   \n",
       "1  The/at Unitarian/jj clergy/nns were/bed an/at ...   \n",
       "2  Ezra/np Stiles/np Gannett/np ,/, an/at honorab...   \n",
       "3  Even/rb so/rb ,/, Gannett/np judiciously/rb ar...   \n",
       "4  We/ppss today/nr are/ber not/* entitled/vbn to...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  Furthermore , as an encouragement to revisioni...   \n",
       "1  The Unitarian clergy were an exclusive club of...   \n",
       "2  Ezra Stiles Gannett , an honorable representat...   \n",
       "3  Even so , Gannett judiciously argued , the Ass...   \n",
       "4  We today are not entitled to excoriate honest ...   \n",
       "\n",
       "                                       tokenized_pos     label  \n",
       "0  rb , cs at nn in nn nn , pps rb bez jj to vb c...  religion  \n",
       "1  at jj nns bed at jj nn in vbn nns -- cs at nn ...  religion  \n",
       "2  np np np , at jj nn in at nn , vbd ppl rb in a...  religion  \n",
       "3  rb rb , np rb vbd , at nn-tl md rb vb cs np ``...  religion  \n",
       "4  ppss nr ber * vbn to vb jj nns wps vbd np to b...  religion  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:56.719988Z",
     "iopub.status.busy": "2025-11-20T00:27:56.719698Z",
     "iopub.status.idle": "2025-11-20T00:27:56.748109Z",
     "shell.execute_reply": "2025-11-20T00:27:56.747479Z",
     "shell.execute_reply.started": "2025-11-20T00:27:56.719967Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename          0\n",
       "para_id           0\n",
       "sent_id           0\n",
       "raw_text          0\n",
       "tokenized_text    0\n",
       "tokenized_pos     0\n",
       "label             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:56.750127Z",
     "iopub.status.busy": "2025-11-20T00:27:56.749890Z",
     "iopub.status.idle": "2025-11-20T00:27:56.759899Z",
     "shell.execute_reply": "2025-11-20T00:27:56.759279Z",
     "shell.execute_reply.started": "2025-11-20T00:27:56.750111Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Furthermore , as an encouragement to revisionist thinking , it manifestly is fair to admit that any fraternity has a constitutional right to refuse to accept persons it dislikes .'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_df[\"tokenized_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:56.760936Z",
     "iopub.status.busy": "2025-11-20T00:27:56.760611Z",
     "iopub.status.idle": "2025-11-20T00:27:56.770626Z",
     "shell.execute_reply": "2025-11-20T00:27:56.769819Z",
     "shell.execute_reply.started": "2025-11-20T00:27:56.760894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:56.771772Z",
     "iopub.status.busy": "2025-11-20T00:27:56.771534Z",
     "iopub.status.idle": "2025-11-20T00:27:56.789896Z",
     "shell.execute_reply": "2025-11-20T00:27:56.789444Z",
     "shell.execute_reply.started": "2025-11-20T00:27:56.771752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(brown_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:56.791132Z",
     "iopub.status.busy": "2025-11-20T00:27:56.790588Z",
     "iopub.status.idle": "2025-11-20T00:27:57.212091Z",
     "shell.execute_reply": "2025-11-20T00:27:57.211486Z",
     "shell.execute_reply.started": "2025-11-20T00:27:56.791116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "min_frequency = 5\n",
    "whole_df = brown_df\n",
    "all_tokens = []\n",
    "for row in brown_df[\"tokenized_text\"]:\n",
    "    clean_text = re.sub(r\"[^\\w\\s]\", \"\", row.lower())\n",
    "    tokens = clean_text.split()\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "counts = Counter(all_tokens)\n",
    "\n",
    "vocab_words = [w for w, c in counts.items() if c >= min_frequency]\n",
    "\n",
    "vocab_words = sorted(vocab_words, key=lambda w: counts[w], reverse=True)\n",
    "\n",
    "vocab_words.append(\"<UNK>\")\n",
    "\n",
    "vocab = vocab_words\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:57.213112Z",
     "iopub.status.busy": "2025-11-20T00:27:57.212867Z",
     "iopub.status.idle": "2025-11-20T00:27:57.217849Z",
     "shell.execute_reply": "2025-11-20T00:27:57.217243Z",
     "shell.execute_reply.started": "2025-11-20T00:27:57.213093Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14175"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:57.218960Z",
     "iopub.status.busy": "2025-11-20T00:27:57.218586Z",
     "iopub.status.idle": "2025-11-20T00:27:57.230182Z",
     "shell.execute_reply": "2025-11-20T00:27:57.229599Z",
     "shell.execute_reply.started": "2025-11-20T00:27:57.218931Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14175"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:57.231142Z",
     "iopub.status.busy": "2025-11-20T00:27:57.230846Z",
     "iopub.status.idle": "2025-11-20T00:27:57.241098Z",
     "shell.execute_reply": "2025-11-20T00:27:57.240362Z",
     "shell.execute_reply.started": "2025-11-20T00:27:57.231120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class CBOW_Dataset(Dataset):\n",
    "    def __init__(self, df, window_size=2, min_frequency=5, word2idx=None, idx2word=None):\n",
    "        self.data = df\n",
    "        self.window_size = window_size\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word \n",
    "        \n",
    "        \n",
    "\n",
    "        # Step 7: build (context, target) pairs\n",
    "        self.pairs = []\n",
    "        for text in self.data['tokenized_text']:\n",
    "            tokens = re.sub(r\"[^\\w\\s]\", \"\", text.lower()).split()\n",
    "            for i in range(self.window_size, len(tokens) - self.window_size):\n",
    "                context = tokens[i - self.window_size:i] + tokens[i + 1:i + self.window_size + 1]\n",
    "                target = tokens[i]\n",
    "\n",
    "                # convert context and target to indices, using <UNK> if missing\n",
    "                context_ids = [self.word2idx[w] if w in self.word2idx else self.word2idx[\"<UNK>\"] for w in context]\n",
    "                target_id = self.word2idx[target] if target in self.word2idx else self.word2idx[\"<UNK>\"]\n",
    "\n",
    "                self.pairs.append((torch.tensor(context_ids, dtype=torch.long),\n",
    "                                   torch.tensor(target_id, dtype=torch.long)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "    def get_word2idx(self):\n",
    "        return self.word2idx\n",
    "\n",
    "    def get_idx2word(self):\n",
    "        return self.idx2word\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torchvision \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:57.242131Z",
     "iopub.status.busy": "2025-11-20T00:27:57.241856Z",
     "iopub.status.idle": "2025-11-20T00:27:57.254887Z",
     "shell.execute_reply": "2025-11-20T00:27:57.254170Z",
     "shell.execute_reply.started": "2025-11-20T00:27:57.242115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F \n",
    "class CBOW_MODEL(Module): \n",
    "    def __init__(self, vocab_size, embedding_size, context_size):  #context size is how many words around the the center word we're predicting\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_size) #create embedding layer \n",
    "        self.fc1 = nn.Linear(embedding_size* context_size, 128) #check why we do embedding size * context size \n",
    "        self.fc2 = nn.Linear(128, vocab_size) #project to a word within the vocab size\n",
    "    \n",
    "    def forward(self, input):  \n",
    "        input = self.embedding_layer(input) #after doing this, our input will have shape: (batch, context_size, d_model), we need to flatten it before passing into fc1\n",
    "        input = input.view(input.size(0), -1) #flattens the embedding and context size. New tensor is (batch_size, embedding_size * context size). Note that this linear layer is taking a\n",
    "        #two dimensional output, but it's okay. The linear transformation is perforemd on the last dimension of the tensor (in this case, embedding_size * context_size)\n",
    "        input = self.fc1(input)\n",
    "        input = F.relu(input)\n",
    "        logits = self.fc2(input)\n",
    "\n",
    "        logits = F.log_softmax(logits, dim=1)\n",
    "\n",
    "        #input = F.softmax(input, dim=-1) already incorporated in loss \n",
    "        return logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:27:57.256246Z",
     "iopub.status.busy": "2025-11-20T00:27:57.255640Z",
     "iopub.status.idle": "2025-11-20T00:28:08.801694Z",
     "shell.execute_reply": "2025-11-20T00:28:08.801024Z",
     "shell.execute_reply.started": "2025-11-20T00:27:57.256220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = CBOW_Dataset(train_df, window_size=2, min_frequency=5, word2idx=word2idx, idx2word=idx2word)\n",
    "test_dataset = CBOW_Dataset(test_df, window_size=2, min_frequency=5, word2idx=word2idx, idx2word=idx2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:28:08.804819Z",
     "iopub.status.busy": "2025-11-20T00:28:08.804619Z",
     "iopub.status.idle": "2025-11-20T00:28:08.808926Z",
     "shell.execute_reply": "2025-11-20T00:28:08.808116Z",
     "shell.execute_reply.started": "2025-11-20T00:28:08.804804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:28:08.809947Z",
     "iopub.status.busy": "2025-11-20T00:28:08.809659Z",
     "iopub.status.idle": "2025-11-20T00:28:08.821582Z",
     "shell.execute_reply": "2025-11-20T00:28:08.820749Z",
     "shell.execute_reply.started": "2025-11-20T00:28:08.809921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:28:08.822977Z",
     "iopub.status.busy": "2025-11-20T00:28:08.822701Z",
     "iopub.status.idle": "2025-11-20T00:28:09.915502Z",
     "shell.execute_reply": "2025-11-20T00:28:09.914595Z",
     "shell.execute_reply.started": "2025-11-20T00:28:08.822954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:28:09.934238Z",
     "iopub.status.busy": "2025-11-20T00:28:09.933992Z",
     "iopub.status.idle": "2025-11-20T00:28:10.199647Z",
     "shell.execute_reply": "2025-11-20T00:28:10.199036Z",
     "shell.execute_reply.started": "2025-11-20T00:28:09.934220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size = 32, \n",
    "                         num_workers = 2, \n",
    "                         shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:29:20.890790Z",
     "iopub.status.busy": "2025-11-20T00:29:20.890031Z",
     "iopub.status.idle": "2025-11-20T00:29:20.894164Z",
     "shell.execute_reply": "2025-11-20T00:29:20.893315Z",
     "shell.execute_reply.started": "2025-11-20T00:29:20.890765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:29:39.494631Z",
     "iopub.status.busy": "2025-11-20T00:29:39.494022Z",
     "iopub.status.idle": "2025-11-20T00:29:39.498261Z",
     "shell.execute_reply": "2025-11-20T00:29:39.497436Z",
     "shell.execute_reply.started": "2025-11-20T00:29:39.494607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = vocab_size-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T00:29:39.777950Z",
     "iopub.status.busy": "2025-11-20T00:29:39.777411Z",
     "iopub.status.idle": "2025-11-20T00:29:39.829288Z",
     "shell.execute_reply": "2025-11-20T00:29:39.828326Z",
     "shell.execute_reply.started": "2025-11-20T00:29:39.777929Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39/2170443352.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW_MODEL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CBOW_MODEL(vocab_size=vocab_size, embedding_size=250, context_size=4).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T00:28:10.268817Z",
     "iopub.status.idle": "2025-11-20T00:28:10.269148Z",
     "shell.execute_reply": "2025-11-20T00:28:10.268992Z",
     "shell.execute_reply.started": "2025-11-20T00:28:10.268978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T00:28:10.269978Z",
     "iopub.status.idle": "2025-11-20T00:28:10.270279Z",
     "shell.execute_reply": "2025-11-20T00:28:10.270139Z",
     "shell.execute_reply.started": "2025-11-20T00:28:10.270125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_total_steps = len(train_loader) \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (context,target) in enumerate(train_loader): \n",
    "        \n",
    "        context = context.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs = model(context) \n",
    "        loss_value = criterion(outputs, target) \n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if(i+1) % 2000 ==0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss_value.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T00:28:10.271767Z",
     "iopub.status.idle": "2025-11-20T00:28:10.272055Z",
     "shell.execute_reply": "2025-11-20T00:28:10.271949Z",
     "shell.execute_reply.started": "2025-11-20T00:28:10.271934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()  # set model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    vocab_size = len(word2idx)  # size of your vocabulary\n",
    "    n_class_correct = [0 for _ in range(vocab_size)]\n",
    "    n_class_samples = [0 for _ in range(vocab_size)]\n",
    "\n",
    "    for context_idxs, target_idxs in test_loader:\n",
    "        context_idxs = context_idxs.to(device)  # shape: (batch_size, context_size)\n",
    "        target_idxs = target_idxs.to(device)    # shape: (batch_size,)\n",
    "\n",
    "        outputs = model(context_idxs)  # shape: (batch_size, vocab_size)\n",
    "        _, predicted = torch.max(outputs, dim=1)  # predicted word indices\n",
    "\n",
    "        n_samples += target_idxs.size(0)\n",
    "        n_correct += (predicted == target_idxs).sum().item()\n",
    "\n",
    "        # per-word accuracy\n",
    "        for i in range(len(target_idxs)):\n",
    "            label = target_idxs[i].item()\n",
    "            pred = predicted[i].item()\n",
    "            if label == pred:\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    overall_acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Overall accuracy: {overall_acc:.2f}%')\n",
    "\n",
    "    # per-word accuracy (optional, can be slow for large vocab)\n",
    "    for i in range(vocab_size):\n",
    "        if n_class_samples[i] > 0:\n",
    "            word_acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "            print(f'Accuracy of \"{idx2word[i]}\": {word_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T00:28:10.273126Z",
     "iopub.status.idle": "2025-11-20T00:28:10.273407Z",
     "shell.execute_reply": "2025-11-20T00:28:10.273300Z",
     "shell.execute_reply.started": "2025-11-20T00:28:10.273287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model,\"cbow_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T00:28:10.274285Z",
     "iopub.status.idle": "2025-11-20T00:28:10.274499Z",
     "shell.execute_reply": "2025-11-20T00:28:10.274399Z",
     "shell.execute_reply.started": "2025-11-20T00:28:10.274390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embed_layer = model.embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T00:28:10.275697Z",
     "iopub.status.idle": "2025-11-20T00:28:10.275964Z",
     "shell.execute_reply": "2025-11-20T00:28:10.275831Z",
     "shell.execute_reply.started": "2025-11-20T00:28:10.275819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "my_dictionary= idx2word\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open(\"idx2word.pkl\", \"wb\") as file:\n",
    "    pickle.dump(my_dictionary, file)\n",
    "\n",
    "# Load the dictionary back from the file\n",
    "with open(\"idx2word.pkl\", \"rb\") as file:\n",
    "    loaded_dictionary = pickle.load(file)\n",
    "\n",
    "print(loaded_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T00:28:10.276600Z",
     "iopub.status.idle": "2025-11-20T00:28:10.276816Z",
     "shell.execute_reply": "2025-11-20T00:28:10.276724Z",
     "shell.execute_reply.started": "2025-11-20T00:28:10.276714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def closest_words(model, word, word_to_idx, idx_to_word, top_k=10):\n",
    "    # 1. look up index of the word\n",
    "    if word not in word_to_idx:\n",
    "        return f\"'{word}' not in vocabulary\"\n",
    "\n",
    "    word_idx = word_to_idx[word]\n",
    "\n",
    "    # 2. get embedding for the query word (1 x d)\n",
    "    word_vec = model.embedding_layer.weight[word_idx]  # (d,)\n",
    "    word_vec = word_vec.unsqueeze(0)                   # (1, d)\n",
    "\n",
    "    # 3. get all embeddings (vocab_size x d)\n",
    "    all_embeds = model.embedding_layer.weight          # (V, d)\n",
    "\n",
    "    # 4. compute cosine similarity with every word\n",
    "    sims = F.cosine_similarity(word_vec, all_embeds)   # (V,)\n",
    "\n",
    "    # 5. sort by similarity (descending)\n",
    "    topk = torch.topk(sims, top_k + 1)                 # +1 to skip the word itself\n",
    "\n",
    "    # 6. convert indices back to words\n",
    "    results = []\n",
    "    for idx in topk.indices:\n",
    "        w = idx_to_word[int(idx)]\n",
    "        if w != word:   # skip the query word\n",
    "            results.append(w)\n",
    "\n",
    "        if len(results) == top_k:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T00:28:10.278182Z",
     "iopub.status.idle": "2025-11-20T00:28:10.278392Z",
     "shell.execute_reply": "2025-11-20T00:28:10.278297Z",
     "shell.execute_reply.started": "2025-11-20T00:28:10.278288Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = closest_words(model, \"the\", word2idx, idx2word, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T00:28:10.279158Z",
     "iopub.status.idle": "2025-11-20T00:28:10.279383Z",
     "shell.execute_reply": "2025-11-20T00:28:10.279268Z",
     "shell.execute_reply.started": "2025-11-20T00:28:10.279259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T00:28:10.280482Z",
     "iopub.status.idle": "2025-11-20T00:28:10.280774Z",
     "shell.execute_reply": "2025-11-20T00:28:10.280647Z",
     "shell.execute_reply.started": "2025-11-20T00:28:10.280634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2058,
     "sourceId": 131078,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
